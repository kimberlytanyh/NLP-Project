{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load packages\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import itertools\n",
    "import collections\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import urllib\n",
    "import urllib.request\n",
    "import re\n",
    "import string\n",
    "\n",
    "%pip install newspaper3k\n",
    "import newspaper\n",
    "import os\n",
    "\n",
    "%pip install tweepy\n",
    "import tweepy as tw\n",
    "from bs4 import BeautifulSoup\n",
    "import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import MultinomialNB  # maybe compare performance with NLTK's, if I can get it to work\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "# set max display of rows to 100\n",
    "pd.set_option('display.max_rows', 120)\n",
    "\n",
    "# show all columns\n",
    "pd.set_option('max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading in our intital disaster tweets dataset into Jupyter NB ennvironment for analysis ;\n",
    "\n",
    "disaster_tweets = pd.read_csv(\"tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finding the number of rows / columns witihin our dataset read in ;\n",
    "\n",
    "disaster_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assessing if any of the columns in our dataset has missing / null values as a means of initial \n",
    "## data exploration ;\n",
    "\n",
    "disaster_tweets.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking out our dataset in dataframe format to visualize and plan for the analysis ;\n",
    "\n",
    "disaster_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Closer examination of the keyword column - we observe that two-word keywords are seperated \n",
    "## by '%20', which we'll have to get rid off in our data cleaning / exploration process ;\n",
    "\n",
    "disaster_tweets.keyword.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further examination of the keyword column within our dataset ;\n",
    "\n",
    "disaster_tweets.keyword.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Devising a simple function to clean up the keyword column, replacing the '%20' in between\n",
    "## two-word keywords with a single splace ;\n",
    "\n",
    "def keyword_clean(text):\n",
    "    \n",
    "    x = re.sub('%20', ' ', text)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Application of our function created above to the keyword column in our dataset to clean it up ;\n",
    "\n",
    "disaster_tweets['keyword'] = disaster_tweets['keyword'].apply(lambda x: keyword_clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking our keyword column cleaning exercise to make sure it worked as planned ;\n",
    "\n",
    "disaster_tweets.keyword.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating dummy variables from our keyword column which are numerical to be used for \n",
    "## our impending modelling analysis ;\n",
    "\n",
    "df_dummies = pd.get_dummies(disaster_tweets, prefix='', prefix_sep='', columns=['keyword'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating an all-purpose function to be used for getting our tweet text column ready to be \n",
    "## applied to CountVectorizer in order to generate even more numerical features for our \n",
    "## impending modelling analysis ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) \n",
    "\n",
    "    text_rc = re.sub('([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)', '', text_lc)\n",
    "    \n",
    "    tokens = re.split('\\W+', text_rc) \n",
    "    \n",
    "    text = [wn.lemmatize(word) for word in tokens if word.isalpha() and word not in stopword]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying Countvectorizer ;\n",
    "\n",
    "countVectorizer = CountVectorizer(analyzer = clean_text) \n",
    "\n",
    "countVector = countVectorizer.fit_transform(disaster_tweets['text'])\n",
    "\n",
    "print('{} Number of tweets has {} words'.format(countVector.shape[0], countVector.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_df = pd.DataFrame(countVector.toarray(), columns = countVectorizer.get_feature_names())\n",
    "\n",
    "count_vect_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concatenating our vectorized dataset from the keyword and text columns together to be used\n",
    "## as features in the modelling analysis ;\n",
    "\n",
    "df = pd.concat([df_dummies, count_vect_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split data into train / validation sets ;\n",
    "\n",
    "train_data, val_data = train_test_split(df, train_size = 0.7, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split the target variable out from the train and validation datasets and drop unneeded columns / variables ;\n",
    "\n",
    "y_train = train_data.target\n",
    "x_train = train_data.drop(columns = [\"target\", \"location\", \"text\"])\n",
    "\n",
    "y_val = val_data.target\n",
    "x_val = val_data.drop(columns = [\"target\", \"location\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO NOT RUN ..\n",
    "## standardizing the columns in the train and validation datasets (takes forever to run, likely omitting) ;\n",
    "\n",
    "#for field in x_train.columns:\n",
    "    #standard_dev = x_train[field].std()\n",
    "    #mean = x_train[field].mean()\n",
    "    \n",
    "    #x_train[field] = (x_train[field] - mean) / standard_dev\n",
    "    #x_val[field] = (x_val[field] - mean) / standard_dev\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtaining the number of rows / columns in our train data ;\n",
    "\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtaining the number of rows / columns in our x_train data ;\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtaining the number of rows / columns in our y_train data - of note, this data has 2 target columns \n",
    "## because of our concatenation exercsise above to combine the created dummies from the keyword to the\n",
    "## features generated from the text column using countvectorizer. Thererfore, we will have to drop\n",
    "## one (1) of those target columns since they are identical ;\n",
    "\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping one of the target columns in our y_train data ;\n",
    "\n",
    "y_train = y_train.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Doing the same exercise for our y_val data as our y_train data above of dropping \n",
    "## one of the duplicated identical target columns ;\n",
    "\n",
    "y_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = y_val.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Logistic Regression Model ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model = LogisticRegression(random_state = 3).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model.predict_proba(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = logit_model.predict(x_train)\n",
    "\n",
    "y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = logit_model.predict(x_val)\n",
    "\n",
    "y_val_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of the Evaluation Metrics for the LogReg model ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy for train ;\n",
    "\n",
    "metrics.accuracy_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy for validation ;\n",
    "\n",
    "metrics.accuracy_score(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Precision for train ;\n",
    "\n",
    "metrics.precision_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Precision for validation ;\n",
    "\n",
    "metrics.precision_score(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recall for train ;\n",
    "\n",
    "metrics.recall_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recall for validation ;\n",
    "\n",
    "metrics.recall_score(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## F1-score for train ;\n",
    "\n",
    "metrics.f1_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## F1-score for validation ;\n",
    "\n",
    "metrics.f1_score(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confusion Matrix for train ;\n",
    "\n",
    "metrics.confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confusion Matrix for validation ;\n",
    "\n",
    "metrics.confusion_matrix(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Random Forest Model ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_model = RandomForestClassifier(n_estimators = 200, max_depth = 3, \n",
    "                                      min_samples_split = 50, min_samples_leaf = 10,\n",
    "                                      max_samples = 0.2, random_state = 0).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get feature importances, which is a normalized measure of the average Gini reduction across the trees in the forest ;\n",
    "\n",
    "forest_model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print a dictionary that maps each feature to its importance level - this is normalized\n",
    "## The higher the feature importance, the more important the feature is considered in the model\n",
    "\n",
    "{var : imp for var, imp in zip(x_train.columns, forest_model.feature_importances_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test train data predicted probabilties ;\n",
    "\n",
    "y_train_prob = forest_model.predict_proba(x_train)\n",
    "\n",
    "y_train_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test validation data predicted probabilities ;\n",
    "\n",
    "y_val_prob = forest_model.predict_proba(x_val)\n",
    "\n",
    "y_val_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred2 = forest_model.predict(x_train)\n",
    "\n",
    "y_train_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred2 = forest_model.predict(x_val)\n",
    "\n",
    "y_val_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculation of the Evaluation Metrics for the Random Forest model ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy for train ;\n",
    "\n",
    "metrics.accuracy_score(y_train, y_train_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy for validation ;\n",
    "\n",
    "metrics.accuracy_score(y_val, y_val_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Precision for train ;\n",
    "\n",
    "metrics.precision_score(y_train, y_train_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Precision for validation ;\n",
    "\n",
    "metrics.precision_score(y_val, y_val_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recall for train ;\n",
    "\n",
    "metrics.recall_score(y_train, y_train_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recall for validation ;\n",
    "\n",
    "metrics.recall_score(y_val, y_val_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## F1-score for train ;\n",
    "\n",
    "metrics.f1_score(y_train, y_train_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## F1-score for validation ;\n",
    "\n",
    "metrics.f1_score(y_val, y_val_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confusion Matrix for train ;\n",
    "\n",
    "metrics.confusion_matrix(y_train, y_train_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confusion Matrix for validation ;\n",
    "\n",
    "metrics.confusion_matrix(y_val, y_val_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AUC on train ;\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_train, y_train_prob[:,1], pos_label = 1)\n",
    "\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC on validation ;\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_val, y_val_prob[:,1], pos_label = 1)\n",
    "\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going an extra step to test both the Logistic Regression and Random Forest models above against actual Twitter data from scraped tweets ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting our Access keys and tokens obtained from a twitter developer account ;\n",
    "\n",
    "consumer_key = '9YIL2z1kPwLUxvjrnYwM9XMDI'\n",
    "\n",
    "consumer_secret = 'Vqx0zTAruVpOuTZmeUETm4aMCMSzI65St4S2bNk1KajOA4CrCj'\n",
    "\n",
    "access_token = '934099208-93iTijdRZkdCPcbudOpGDUzsvCwKSqcP3ObWgyrC'\n",
    "\n",
    "access_token_secret = 'wG2UwDxjWFXiyHfFLMhj94164DkOul9sdVmphuokdQRxx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assigning our authentication for scraping twitter with the API keys and tokens set forth above ;\n",
    "\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scraping tweets from Twitter related to disasters with the search term / hashtag #coronavirus ;\n",
    "\n",
    "search_term = '#coronavirus -filter:retweets'\n",
    "\n",
    "tweets = tw.Cursor(api.search,\n",
    "                   q = search_term,\n",
    "                   lang = 'en',\n",
    "                   since = '2019-10-01').items(3000)\n",
    "\n",
    "tweets_data1 = [tweet.text for tweet in tweets]\n",
    "\n",
    "tweets_data1[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing URL links and other ancillary elements from our tweets data obtained above using Regular Expressions ;\n",
    "\n",
    "def remove_url(txt):\n",
    "    \n",
    "    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", txt).split())\n",
    "\n",
    "\n",
    "tweets_data_no_urls1 = [remove_url(tweet) for tweet in tweets_data1]\n",
    "\n",
    "tweets_data_no_urls1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scraping tweets from Twitter related to disasters with the search term / hashtag #bushfires ;\n",
    "\n",
    "search_term = '#bushfires -filter:retweets'\n",
    "\n",
    "tweets = tw.Cursor(api.search,\n",
    "                   q = search_term,\n",
    "                   lang = 'en',\n",
    "                   since = '2019-10-01').items(3000)\n",
    "\n",
    "tweets_data2 = [tweet.text for tweet in tweets]\n",
    "\n",
    "tweets_data2[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing URL links and other ancillary elements from our tweets data obtained above using Regular Expressions ;\n",
    "\n",
    "def remove_url(txt):\n",
    "    \n",
    "    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", txt).split())\n",
    "\n",
    "\n",
    "tweets_data_no_urls2 = [remove_url(tweet) for tweet in tweets_data2]\n",
    "\n",
    "tweets_data_no_urls2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scraping tweets from Twitter related to disasters with the search term / hashtag #volcanoeruption ;\n",
    "\n",
    "search_term = '#volcanoeruption -filter:retweets'\n",
    "\n",
    "tweets = tw.Cursor(api.search,\n",
    "                   q = search_term,\n",
    "                   lang = 'en',\n",
    "                   since = '2019-10-01').items(3000)\n",
    "\n",
    "tweets_data3 = [tweet.text for tweet in tweets]\n",
    "\n",
    "tweets_data3[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing URL links and other ancillary elements from our tweets data obtained above using Regular Expressions ;\n",
    "\n",
    "def remove_url(txt):\n",
    "    \n",
    "    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", txt).split())\n",
    "\n",
    "\n",
    "tweets_data_no_urls3 = [remove_url(tweet) for tweet in tweets_data3]\n",
    "\n",
    "tweets_data_no_urls3[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a new dataset from the raw scraped tweets to be used for manual mapping to a target column ;\n",
    "\n",
    "df5 = pd.DataFrame(data = zip(tweets_data1), \n",
    "                   columns = ['tweet'])\n",
    "\n",
    "df5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a new column - keyword, which is derived from the searchterm / hashtag from which the tweets \n",
    "## were scraped from Twitter ;\n",
    "\n",
    "df5['keyword'] = \"coronavirus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a new dataset from the raw scraped tweets to be used for manual mapping to a target column ;\n",
    "\n",
    "df6 = pd.DataFrame(data = zip(tweets_data2), \n",
    "                   columns = ['tweet'])\n",
    "\n",
    "df6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a new column - keyword, which is derived from the searchterm / hashtag from which the tweets \n",
    "## were scraped from Twitter ;\n",
    "\n",
    "df6['keyword'] = \"bushfires\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a new dataset from the raw scraped tweets to be used for manual mapping to a target column ;\n",
    "\n",
    "df7 = pd.DataFrame(data = zip(tweets_data3), \n",
    "                   columns = ['tweet'])\n",
    "\n",
    "df7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a new column - keyword, which is derived from the searchterm / hashtag from which the tweets \n",
    "## were scraped from Twitter ;\n",
    "\n",
    "df7['keyword'] = \"volcanoeruption\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concatenating the three (3) sets of tweets data scraped into a single dataframe to be used for the test set ;\n",
    "\n",
    "df_tweets_url = pd.concat([df5, df6, df7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_url.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_url.tweet.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Taking a random sample of 120 tweets from our created dataframe to be used for the manual mapping to a \n",
    "## target column for analysis ;\n",
    "\n",
    "df_analysis_url = df_tweets_url.sample(120, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating a CSV file from our sampled datafram for analysis in order to manually map each tweet to \n",
    "## a target column, as either disaster - 1 or non-disaster - 0 ;\n",
    "\n",
    "df_analysis_url.to_csv(\"test_tweets_url.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets = pd.read_csv(\"test_tweets_url_mapped.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping the ancillary first column 'Unnamed: 0' which was generated from the initial index of the \n",
    "## test_tweets_url CSV generated for the mapping ;\n",
    "\n",
    "test_tweets = test_tweets.drop(columns = [\"Unnamed: 0\"])\n",
    "\n",
    "test_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_dummies_tweets = pd.get_dummies(test_tweets, prefix = '', prefix_sep = '', columns=['keyword'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) \n",
    "\n",
    "    text_rc = re.sub('([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)', '', text_lc)\n",
    "    \n",
    "    tokens = re.split('\\W+', text_rc) \n",
    "    \n",
    "    text = [wn.lemmatize(word) for word in tokens if word.isalpha() and word not in stopword]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying Countvectorizer ;\n",
    "\n",
    "countVectorizer = CountVectorizer(analyzer = clean_text) \n",
    "\n",
    "countVector = countVectorizer.fit_transform(test_tweets['tweet'])\n",
    "\n",
    "print('{} Number of tweets has {} words'.format(countVector.shape[0], countVector.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_count_vect_df = pd.DataFrame(countVector.toarray(), columns = countVectorizer.get_feature_names())\n",
    "\n",
    "tweets_count_vect_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_count_vect_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.concat([df_dummies_tweets, tweets_count_vect_df], axis = 1)\n",
    "\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = tweets_df.target\n",
    "\n",
    "x_test = tweets_df.drop(columns = [\"target\", \"tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying our Logistic Regression model to the scraped tweets test data ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model2 = LogisticRegression(random_state = 5).fit(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model2.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = logit_model2.predict(x_test)\n",
    "\n",
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy for test ;\n",
    "\n",
    "metrics.accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Precision for test ;\n",
    "\n",
    "metrics.precision_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recall for test ;\n",
    "\n",
    "metrics.recall_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## F1-score for test ;\n",
    "\n",
    "metrics.f1_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confusion Matrix for test ;\n",
    "\n",
    "metrics.confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying our Random Forest model to the scraped tweets test data ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_model2 = RandomForestClassifier(n_estimators = 100, max_depth = 3, \n",
    "                                      min_samples_split = 50, min_samples_leaf = 10,\n",
    "                                      max_samples = 0.2, random_state = 0).fit(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get feature importances, which is a normalized measure of the average Gini reduction across the trees in the forest ;\n",
    "\n",
    "forest_model2.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print a dictionary that maps each feature to its importance level - this is normalized\n",
    "## The higher the feature importance, the more important the feature is considered in the model\n",
    "\n",
    "{var : imp for var, imp in zip(x_test.columns, forest_model2.feature_importances_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = pd.DataFrame(disaster_tweets.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing Punctuation ;\n",
    "\n",
    "def remove_punct(text):\n",
    "\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "    text = re.sub('[0â€“9]+', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "df_text['punct'] = df_text['text'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "\n",
    "## Applying tokenization ;\n",
    "\n",
    "def tokenization(text):\n",
    "\n",
    "    text = re.split('\\W+', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "df_text['tokenized'] = df_text['punct'].apply(lambda x: tokenization(x.lower()))\n",
    "\n",
    "## Removing stopwords ;\n",
    "\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "\n",
    "    text = [word for word in text if word not in stopword]\n",
    "\n",
    "    return text\n",
    "\n",
    "df_text['nonstopwords'] = df_text['tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "## Applying Lemmatizer ;\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemma(text):\n",
    "\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "\n",
    "    return text\n",
    "\n",
    "df_text['lemmatized'] = df_text['nonstopwords'].apply(lambda x: lemma(x))\n",
    "\n",
    "\n",
    "## df_text.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtweets = pd.read_csv(\"tweets.csv\", index_col = 0) # id same as index, use id column as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null count\n",
    "dtweets.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code for removing links\n",
    "sentence = [\"The\", \"weather\", \"(https:dfsdfsdfs)\", \"has\", \"been\", \"good\"]\n",
    "sentence = [word for word in sentence if not re.search('^.?http.+$', word)]\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtweets.location.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove location column - too many missing and has invalid locations\n",
    "del dtweets[\"location\"]\n",
    "dtweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtweets['text'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code for including words with training '...', but not including links\n",
    "sent = [\"set\", \"ablaze...\"]\n",
    "[re.sub(\"(\\.)+\", \"\", word) for word in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write function to clean text column\n",
    "\n",
    "def clean_text(document):    # document refers to an individual tweet\n",
    "    tokens = nltk.tokenize.word_tokenize(document)\n",
    "    # sub trailing '...' with \" \" - imperfect tokenization\n",
    "    clean_tokens = [re.sub(\"(\\.)+\", \"\", word) for word in tokens]\n",
    "    # remove non-alphanum chars such as emoticons and punctuations\n",
    "    clean_tokens = [word.lower() for word in clean_tokens if word.isalnum()]\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    clean_tokens = [word for word in clean_tokens if word not in stopwords] # remove stopwords\n",
    "    # remove links \n",
    "    clean_tokens = [word for word in clean_tokens if not re.search('^.?http.+$', word)]\n",
    "    # perform lemmatization\n",
    "    wn = nltk.WordNetLemmatizer()\n",
    "    clean_tokens = [wn.lemmatize(token) for token in clean_tokens]\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtweets['text'] = dtweets['text'].map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtweets['text'].iloc[11367]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of length of tweets for different targets\n",
    "dtweet_yes = dtweets[dtweets[\"target\"] == 1]\n",
    "dtweet_no = dtweets[dtweets[\"target\"] == 0]\n",
    "\n",
    "fdisty = nltk.FreqDist(len(sent) for sent in dtweet_yes[\"text\"])\n",
    "fdistn = nltk.FreqDist(len(sent) for sent in dtweet_no[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dtweet_yes))\n",
    "fdisty.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dtweet_no))\n",
    "fdisty.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can't see any relationship between tweet length and target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspecting first fews row to see keyword and text relationship\n",
    "dtweets['text'].iloc[0]   # where is ablaze?\n",
    "# keyword is a criteria used in search to retrieve tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unique keywords\n",
    "dtweets[\"keyword\"].unique()     #weird \"%20\" between keywords with two or more words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write function to sub \"%20\" with \" \"\n",
    "\n",
    "def clean_keyword(document):\n",
    "    cleaned = re.sub(\"%20\", \" \", document)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map clean_keyword function to keyword column\n",
    "\n",
    "dtweets['keyword'] = dtweets['keyword'].map(clean_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy variables for keyword column\n",
    "dtweets = pd.get_dummies(dtweets, columns=[\"keyword\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect first few rows to see if combination was successful\n",
    "dtweets.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (x) from label (y)\n",
    "from sklearn import utils\n",
    "dtweets = utils.shuffle(dtweets, random_state=123)\n",
    "x = dtweets.drop(columns = [\"target\"])\n",
    "y = dtweets[\"target\"]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and validation set (70-30)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, train_size = 0.7, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer a.k.a Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "count_vec = CountVectorizer(\n",
    "    analyzer='word', tokenizer=dummy, preprocessor=dummy,\n",
    "    token_pattern=None, ngram_range = (1,1), lowercase = False) \n",
    "\n",
    "# get transformed training data text column\n",
    "train_count = count_vec.fit_transform(x_train[\"text\"])\n",
    "bow_train = pd.DataFrame(train_count.toarray(), columns = count_vec.get_feature_names())\n",
    "bow_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word_count variable to reduce feature size to important words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = bow_train.sum()\n",
    "word_counts = word_counts.sort_values(ascending = False)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_bow_train = bow_train[word_counts[word_counts>=20].index] #trying 20 \n",
    "reduced_bow_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Set 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join reduced_bow_train to x_train (create feature set 1)\n",
    "# unstandardized data\n",
    "\n",
    "x_train1 = pd.concat([x_train.reset_index(drop = True), reduced_bow_train.reset_index(drop=True)], axis = 1)\n",
    "x_train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply transformation to validation data\n",
    "\n",
    "val_count = count_vec.transform(x_val['text'])\n",
    "bow_val = pd.DataFrame(val_count.toarray(), columns = count_vec.get_feature_names())\n",
    "reduced_bow_val = bow_val[reduced_bow_train.columns]\n",
    "print(len(bow_val))\n",
    "\n",
    "x_val1 = pd.concat([x_val.reset_index(drop = True), reduced_bow_val.reset_index(drop=True)], axis = 1)\n",
    "x_val1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'text' column in x_train1 and x_val1\n",
    "\n",
    "del x_train1[\"text\"]\n",
    "del x_val1[\"text\"]\n",
    "x_val1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create TF-IDF vectorizer\n",
    "\n",
    "tf_idf = TfidfVectorizer(\n",
    "    analyzer='word', tokenizer=dummy, preprocessor=dummy,\n",
    "    token_pattern=None, ngram_range = (1,1), lowercase = False) \n",
    "\n",
    "# get transformed training data text column\n",
    "train_tf_idf = tf_idf.fit_transform(x_train[\"text\"])\n",
    "train_tf_idf = pd.DataFrame(train_tf_idf.toarray(), columns = tf_idf.get_feature_names())\n",
    "train_tf_idf.head()\n",
    "\n",
    "train_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Set 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get reduced feature sets using word_counts\n",
    "reduced_train_tfidf = train_tf_idf[word_counts[word_counts >= 20].index]\n",
    "reduced_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2 = pd.concat([x_train.reset_index(drop = True), reduced_train_tfidf.reset_index(drop=True)], axis = 1)\n",
    "x_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tf-idf transformation to validation dataset\n",
    "\n",
    "val_tf_idf = tf_idf.transform(x_val[\"text\"])\n",
    "val_tf_idf = pd.DataFrame(val_tf_idf.toarray(), columns = tf_idf.get_feature_names())\n",
    "reduced_val_tfidf = val_tf_idf[word_counts[word_counts >= 20].index]\n",
    "\n",
    "x_val2 = pd.concat([x_val.reset_index(drop = True), reduced_val_tfidf.reset_index(drop=True)], axis = 1)\n",
    "x_val2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete text column from x_train2 and x_val2\n",
    "\n",
    "del x_train2[\"text\"]\n",
    "del x_val2['text']\n",
    "x_val2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec (Too many NAs - Don't use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim.models import Word2Vec\n",
    "#import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start = time.time()\n",
    "#w2v = Word2Vec(x_train, vector_size = 300, window = 4)\n",
    "#end = time.time()\n",
    "\n",
    "#print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average embeddings for each document\n",
    "\n",
    "# create empty list to hold average embeddings each document\n",
    "#avg_embeddings = []\n",
    "\n",
    "#from tqdm import tqdm\n",
    "\n",
    "# loop over each doc\n",
    "#for index in tqdm(range(x_train.shape[0])):\n",
    "    #doc = x_train[\"text\"].iloc[index]\n",
    "    #embeddings = [w2v.wv[word] for word in doc if word in w2v.wv]\n",
    "    \n",
    "    #if embeddings == []:\n",
    "        #avg_embeddings.append([np.nan]*300)\n",
    "    #else:\n",
    "        #avg = np.mean(embeddings, axis = 0)\n",
    "        #avg_embeddings.append(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#avg_embeddings = [array if isinstance(array, list) else array.tolist() for array in avg_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v_features = pd.DataFrame(avg_embeddings)\n",
    "#w2v_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Set 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn Naive Bayes\n",
    "\n",
    "# Feature Set 1\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_model1 = nb_classifier.fit(x_train1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = nb_model1.predict(x_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Set\n",
    "metrics.confusion_matrix(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance on Training Set\n",
    "\n",
    "print(\"Training Set Performance of nb_model1\")\n",
    "accuracy_train1 = metrics.accuracy_score(y_train, y_pred_train)\n",
    "print(\"Accuracy score:\", accuracy_train1)\n",
    "\n",
    "precision_train1 = metrics.precision_score(y_train, y_pred_train)\n",
    "print(\"Precision score:\", precision_train1)\n",
    "\n",
    "recall_train1 = metrics.recall_score(y_train, y_pred_train)\n",
    "print(\"Recall score:\", recall_train1)\n",
    "\n",
    "f1_train1 = metrics.f1_score(y_train, y_pred_train)\n",
    "print(\"F-1 score:\", f1_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_prob = nb_model1.predict_proba(x_train1)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_train, y_train_prob[:,1], pos_label = 1)\n",
    "print(\"AUC score:\", metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val = nb_model1.predict(x_val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance on Validation Set\n",
    "\n",
    "print(\"Validation Set Performance of nb_model1:\")\n",
    "accuracy_val1 = metrics.accuracy_score(y_val, y_pred_val)\n",
    "print(\"Accuracy score:\", accuracy_val1)\n",
    "\n",
    "precision_val1 = metrics.precision_score(y_val, y_pred_val)\n",
    "print(\"Precision score:\", precision_val1)\n",
    "\n",
    "recall_val1 = metrics.recall_score(y_val, y_pred_val)\n",
    "print(\"Recall score:\", recall_val1)\n",
    "\n",
    "f1_val1 = metrics.f1_score(y_val, y_pred_val)\n",
    "print(\"F-1 score:\", f1_val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_prob = nb_model1.predict_proba(x_val1)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_val, y_val_prob[:,1], pos_label = 1)\n",
    "print(\"AUC score for validation set:\", metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Set 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Set 2\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_model2 = nb_classifier.fit(x_train2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train2 = nb_model1.predict(x_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Set\n",
    "metrics.confusion_matrix(y_train, y_pred_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance on Training Set\n",
    "\n",
    "print(\"Training Set Performance of nb_model2\")\n",
    "accuracy_train2 = metrics.accuracy_score(y_train, y_pred_train2)\n",
    "print(\"Accuracy score:\", accuracy_train2)\n",
    "\n",
    "precision_train2 = metrics.precision_score(y_train, y_pred_train2)\n",
    "print(\"Precision score:\", precision_train2)\n",
    "\n",
    "recall_train2 = metrics.recall_score(y_train, y_pred_train2)\n",
    "print(\"Recall score:\", recall_train2)\n",
    "\n",
    "f1_train2 = metrics.f1_score(y_train, y_pred_train2)\n",
    "print(\"F-1 score:\", f1_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_prob2 = nb_model2.predict_proba(x_train2)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_train, y_train_prob2[:,1], pos_label = 1)\n",
    "print(\"AUC score:\", metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val2 = nb_model2.predict(x_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance on Validation Set\n",
    "\n",
    "print(\"Validation Set Performance of nb_model2:\")\n",
    "accuracy_val2 = metrics.accuracy_score(y_val, y_pred_val2)\n",
    "print(\"Accuracy score:\", accuracy_val2)\n",
    "\n",
    "precision_val2 = metrics.precision_score(y_val, y_pred_val2)\n",
    "print(\"Precision score:\", precision_val2)\n",
    "\n",
    "recall_val2 = metrics.recall_score(y_val, y_pred_val2)\n",
    "print(\"Recall score:\", recall_val2)\n",
    "\n",
    "f1_val2 = metrics.f1_score(y_val, y_pred_val2)\n",
    "print(\"F-1 score:\", f1_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_prob2 = nb_model2.predict_proba(x_val2)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_val, y_val_prob2[:,1], pos_label = 1)\n",
    "print(\"AUC score for validation set:\", metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk Naive Bayes\n",
    "\n",
    "# convert dataframe to dictionary\n",
    "# x_train1_dict = pd.DataFrame.to_dict(x_train1)\n",
    "# x_train1_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBC1 = nltk.NaiveBayesClassifier.train(x_train1_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparamater tuning using Randomized Search CV - interrupted kernal because it is taking a while\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "parameters = {\n",
    "        'max_depth': range(2, 6),\n",
    "        'n_estimators': [50, 100, 150, 200, 250, 300],\n",
    "        'subsample': [0.6, 0.7, 0.8],\n",
    "        'colsample_bytree': [0.2, 0.4, 0.5, 0.6, 0.7, 0.8, 1],\n",
    "        'colsample_bynode': [0.2, 0.4, 0.5, 0.6, 0.7, 0.8, 1],\n",
    "        'gamma': [0, 5, 10, 15, 20],\n",
    "        'learning_rate': [0.05, 0.1, 0.15, 0.2, 0.25, 0.3],\n",
    "        'lambda': [0.1, 0.25, 0.5, 0.75, 1]\n",
    "}\n",
    "\n",
    "classifier = RandomizedSearchCV(xgb.XGBClassifier(use_label_encoder = False, eval_metric = \"logloss\"), \n",
    "                         parameters, n_jobs=4, scoring = \"roc_auc\", n_iter = 300,\n",
    "                         random_state = 123)\n",
    "\n",
    "xgb_model1 = classifier.fit(x_train1, y_train)\n",
    "xgb_model2 = classifier.fit(x_train2, y_train)\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.DataFrame()\n",
    "feature_imp[\"feature\"] = x_train1.columns\n",
    "feature_imp[\"importance\"] = xgb_model1.best_estimator_.feature_importances_\n",
    "\n",
    "feature_imp = feature_imp.sort_values(\"importance\", ascending = False).reset_index(drop = True)\n",
    "feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.DataFrame()\n",
    "feature_imp[\"feature\"] = x_train2.columns\n",
    "feature_imp[\"importance\"] = xgb_model2.best_estimator_.feature_importances_\n",
    "\n",
    "feature_imp = feature_imp.sort_values(\"importance\", ascending = False).reset_index(drop = True)\n",
    "feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Set 1\n",
    "\n",
    "y_train_prob3 = xgb_model1.predict_proba(x_train1)\n",
    "y_val_prob3 = xgb_model1.predict_proba(x_val1)\n",
    "\n",
    "# AUC\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_train, y_train_prob3[:,1], pos_label = 1)\n",
    "print(\"AUC for Feature Set 1 train set:\", metrics.auc(fpr, tpr))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_val, y_val_prob3[:,1], pos_label = 1)\n",
    "print(\"AUC for Feature Set 1 validation set:\", metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine xgb_model1's performance when we maximixe on F1-score\n",
    "\n",
    "threshold = np.arange(0.01, 1, .01) # create thresholds\n",
    "\n",
    "# create empty lists for each metric\n",
    "precision_values = []\n",
    "recall_values = []\n",
    "acc_values = []\n",
    "f1_values = []\n",
    "\n",
    "# Loop through each threshold value\n",
    "for value in threshold:\n",
    "    \n",
    "    # get 1 / 0 predictions based off probability threshold\n",
    "    pred = [1 if prob >= value else 0 for prob in y_train_prob3[:,1]]\n",
    "    \n",
    "    # calculate precision, recall, accuracy, and f1-score\n",
    "    precision = metrics.precision_score(y_train, pred)\n",
    "    recall = metrics.recall_score(y_train, pred)\n",
    "    accuracy = metrics.accuracy_score(y_train, pred)\n",
    "    f1_score = metrics.f1_score(y_train, pred)\n",
    "    \n",
    "    # add precision, recall, accuracy, and f1-score to their respective lists\n",
    "    precision_values.append(precision)\n",
    "    recall_values.append(recall)\n",
    "    acc_values.append(accuracy)\n",
    "    f1_values.append(f1_score)\n",
    "\n",
    "# put precision, recall, and accuracy values into a data frame\n",
    "result = pd.DataFrame()\n",
    "result[\"threshold\"] = threshold\n",
    "result[\"precision\"] = precision_values\n",
    "result[\"recall\"] = recall_values\n",
    "result[\"accuracy\"] = acc_values\n",
    "result[\"f1_score\"] = f1_values\n",
    "\n",
    "result.iloc[result.f1_score.idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = [1 if prob >= 0.28 else 0 for prob in y_val_prob3[:,1]]\n",
    "\n",
    "# calculate precision, recall, accuracy, and f1-score\n",
    "precision = metrics.precision_score(y_val, pred)\n",
    "recall = metrics.recall_score(y_val, pred)\n",
    "accuracy = metrics.accuracy_score(y_val, pred)\n",
    "f1_score = metrics.f1_score(y_val, pred)\n",
    "\n",
    "print(\"Validation precision: \", precision)\n",
    "print(\"Validation recall: \", recall)\n",
    "print(\"Validation accuracy: \", accuracy)\n",
    "print(\"Validation F1-Score: \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizing recall - precision, accuracy, etc. too low (don't want that either)\n",
    "\n",
    "result.iloc[result.recall.idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot relationship between percision and recall score\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Precision and Recall Scores vs. Probability Threshold\")\n",
    "plt.plot(result.threshold, result.precision, \"b--\", label=\"Precision\")\n",
    "plt.plot(result.threshold, result.recall, \"g-\", label=\"Recall\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Probability Threshold\")\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automated threshold\n",
    "\n",
    "y_train_pred3 = xgb_model1.predict(x_train1)\n",
    "y_val_pred3 = xgb_model1.predict(x_val1)\n",
    "\n",
    "precision = metrics.precision_score(y_train, y_train_pred3)\n",
    "recall = metrics.recall_score(y_train, y_train_pred3)\n",
    "accuracy = metrics.accuracy_score(y_train, y_train_pred3)\n",
    "f1_score = metrics.f1_score(y_train, y_train_pred3)\n",
    "\n",
    "print(\"xgb_model1 Performance on Train Set:\")\n",
    "print(\"Train precision: \", precision)\n",
    "print(\"Train recall: \", recall)\n",
    "print(\"Train accuracy: \", accuracy)\n",
    "print(\"Train F1-Score: \", f1_score)\n",
    "\n",
    "precision = metrics.precision_score(y_val, y_val_pred3)\n",
    "recall = metrics.recall_score(y_val, y_val_pred3)\n",
    "accuracy = metrics.accuracy_score(y_val, y_val_pred3)\n",
    "f1_score = metrics.f1_score(y_val, y_val_pred3)\n",
    "\n",
    "print(\"\\n\\nxgb_model1 Performance on Validation Set:\")\n",
    "print(\"Validation precision: \", precision)\n",
    "print(\"Validation recall: \", recall)\n",
    "print(\"Validation accuracy: \", accuracy)\n",
    "print(\"Validation F1-Score: \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Set 2\n",
    "\n",
    "y_train_prob4 = xgb_model2.predict_proba(x_train2)\n",
    "y_val_prob4 = xgb_model2.predict_proba(x_val2)\n",
    "\n",
    "# AUC\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_train, y_train_prob4[:,1], pos_label = 1)\n",
    "print(\"AUC for Feature Set 2 train set:\", metrics.auc(fpr, tpr))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_val, y_val_prob4[:,1], pos_label = 1)\n",
    "print(\"AUC for Feature Set 2 validation set:\", metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine xgb_model2's performance when we maximixe on F1-score\n",
    "\n",
    "threshold = np.arange(0.01, 1, .01) # threshold values\n",
    "\n",
    "# create empty lists for each metric\n",
    "precision_values = []\n",
    "recall_values = []\n",
    "acc_values = []\n",
    "f1_values = []\n",
    "\n",
    "# Loop through each threshold values\n",
    "for value in threshold:\n",
    "    \n",
    "    # get 1 / 0 predictions based off probability threshold\n",
    "    pred = [1 if prob >= value else 0 for prob in y_train_prob4[:,1]]\n",
    "    \n",
    "    # calculate precision, recall, accuracy, and f1-score\n",
    "    precision = metrics.precision_score(y_train, pred)\n",
    "    recall = metrics.recall_score(y_train, pred)\n",
    "    accuracy = metrics.accuracy_score(y_train, pred)\n",
    "    f1_score = metrics.f1_score(y_train, pred)\n",
    "    \n",
    "    # add precision, recall, accuracy, and f1-score to their respective lists\n",
    "    precision_values.append(precision)\n",
    "    recall_values.append(recall)\n",
    "    acc_values.append(accuracy)\n",
    "    f1_values.append(f1_score)\n",
    "\n",
    "# put precision, recall, and accuracy values into a data frame\n",
    "result = pd.DataFrame()\n",
    "result[\"threshold\"] = threshold\n",
    "result[\"Train precision\"] = precision_values\n",
    "result[\"Train recall\"] = recall_values\n",
    "result[\"Train accuracy\"] = acc_values\n",
    "result[\"Train f1_score\"] = f1_values\n",
    "\n",
    "result.iloc[result[\"Train f1_score\"].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = [1 if prob >= 0.28 else 0 for prob in y_val_prob3[:,1]]\n",
    "\n",
    "# calculate precision, recall, accuracy, and f1-score\n",
    "precision = metrics.precision_score(y_val, pred)\n",
    "recall = metrics.recall_score(y_val, pred)\n",
    "accuracy = metrics.accuracy_score(y_val, pred)\n",
    "f1_score = metrics.f1_score(y_val, pred)\n",
    "\n",
    "print(\"Validation precision: \", precision)\n",
    "print(\"Validation recall: \", recall)\n",
    "print(\"Validation accuracy: \", accuracy)\n",
    "print(\"Validation F1-Score: \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred4 = xgb_model2.predict(x_train2)\n",
    "y_val_pred4 = xgb_model2.predict(x_val2)\n",
    "\n",
    "precision = metrics.precision_score(y_train, y_train_pred4)\n",
    "recall = metrics.recall_score(y_train, y_train_pred4)\n",
    "accuracy = metrics.accuracy_score(y_train, y_train_pred4)\n",
    "f1_score = metrics.f1_score(y_train, y_train_pred4)\n",
    "\n",
    "print(\"xgb_model2 Performance on Train Set:\")\n",
    "print(\"Train precision: \", precision)\n",
    "print(\"Train recall: \", recall)\n",
    "print(\"Train accuracy: \", accuracy)\n",
    "print(\"Train F1-Score: \", f1_score)\n",
    "\n",
    "precision = metrics.precision_score(y_val, y_val_pred4)\n",
    "recall = metrics.recall_score(y_val, y_val_pred4)\n",
    "accuracy = metrics.accuracy_score(y_val, y_val_pred4)\n",
    "f1_score = metrics.f1_score(y_val, y_val_pred4)\n",
    "\n",
    "print(\"\\n\\nxgb_model2 Performance on Validation Set:\")\n",
    "print(\"Validation precision: \", precision)\n",
    "print(\"Validation recall: \", recall)\n",
    "print(\"Validation accuracy: \", accuracy)\n",
    "print(\"Validation F1-Score: \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation: XGBoost models are overfitting a lot. We could perform dimensionality reduction\n",
    "# using PCA if we had more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
